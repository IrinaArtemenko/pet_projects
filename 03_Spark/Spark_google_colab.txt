Для решения задач используется google colaboratory, для проверки потребуется предоставить код-решения либо же сразу блокнот

!apt-get install openjdk-8-jdk-headless -qq > /dev/null 
!wget -q https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz 
!tar xvzf spark-3.4.2-bin-hadoop3.tgz 
 
import os 
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64" 
os.environ["SPARK_HOME"] = "/content/spark-3.4.2-bin-hadoop3" 
 
!wget https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.4.2/spark-avro_2.12-3.4.2.jar  -P $SPARK_HOME/jars/ 
!echo spark.executor.extraClassPath $SPARK_HOME/jars/spark-avro_2.12/3.4.2.jar >>  /content/spark-3.4.2-bin-hadoop3/conf/spark-defaults.conf 
!echo spark.driver.extraClassPath $SPARK_HOME/jars/spark-avro_2.12/3.4.2.jar >>  /content/spark-3.4.2-bin-hadoop3/conf/spark-defaults.conf 
 
!pip install findspark pyspark==3.4.2

import findspark 
findspark.init() 
import os 
import pyspark 
from pyspark.sql import SparkSession 
from pyspark import SparkConf 
from pyspark import  SparkContext 
from pyspark.sql import functions as f 
 
raw_path = r'/content/data_for_exam' 
done_path = r'/content/done_data_for_exam' 
 
spark = SparkSession.builder \ 
                    .enableHiveSupport() \ 
                    .master('local[*]') \ 
                    .appName("PySpark Task 1")\ 
                    .getOrCreate() 


#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#1. Ќапишите запрос, возвращающий строку следующего вида длЯ каждого сотрудника: "<фамилиЯ> зарабатывает <зарплата> каждый месЯц, но хочет получать <зарплата * 3>". 
#Ќазовите колонку 'Dream Salaries'. ђезультат сохранить в формате parquet со сжатием GZIP

df_emp.createOrReplaceTempView("employees")
df = spark.sql("select last_name||' зарабатывает '||salary||' каждый месяц, но хочет получать '||salary*3 as Dream_Salaries from employees")
df.show(3, False)

df.write.format("parquet").option("compression", "gzip").mode("overwrite").save("/content/data_out/task1")

#читаю созданный файл
spark.read.format("parquet").option("compression", "gzip").load("/content/data_out/task1").show(5,False)

#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#2. Ќапишите запрос, возвращающий адреса всех департаментов. ‡апрос должен возвращать ID локации, адрес (улицу), город, штат и страну.

spark.read.format("parquet").option("compression", "gz").load("/content/locations").show(5,False)

df_locations = spark.read.format("parquet").option("codec", "gz").load("/content/locations")
df_countries = spark.read.format("orc").option("codec", "snappy").load("/content/countries")
df_locations.createOrReplaceTempView("locations")
df_countries.createOrReplaceTempView("countries")
df = spark.sql("select l.location_id,l.street_address,l.city,l.state_province,c.country_name\
                  from locations l join countries c on c.country_id=l.country_id")
df.show(20, False)

#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#3. Ќапишите запрос, возвращающий фамилию, ID отдела и наименование отдела длЯ каждого сотрудника; ђезультат сохранить в формате avro со сжатием GZIP



df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_departments = spark.read.format("csv").option("header","true")\
						   .option("inferSchema","true").option("delimiter",",")\
						   .load("/content/departments")
df_employees.createOrReplaceTempView("employees")
df_departments.createOrReplaceTempView("departments")
df = spark.sql("select e.last_name, d.department_id, department_name from employees e\
                  join departments d on d.department_id = e.department_id")
df.show(20, False)

#в avro не существует кодека сжатиЯ GZIP, только snappy и deflate, использовала deflate
df.write.format("avro").option("compression", "deflate").mode("overwrite").save("/content/data_out/task3")

#читаю созданный файл
spark.read.format("avro").option("compression", "deflate").load("/content/data_out/task3").show(5,False)


#zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#4. Ќапишите запрос, возвращающий фамилию, ID сотрудника, фамилию менеджера и ID менеджера длЯ каждого сотрудника (длЯ сотрудников, у которых нет менеджера, в этих колонках должен быть NULL). 
#Ќазовите колонки 'Employee', 'Emp#', 'Manager', 'Mgr#'. ђезультат сохранить в формате avro со сжатием Snappy

df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_employees.createOrReplaceTempView("employees")

df = spark.sql("select e.last_name as Employee, e.employee_id as Emp, em.last_name as Manager, em.employee_id as Mgr\
                 from employees e join employees em on e.manager_id=em.employee_id")
df.show(20, False)

#в avro кодек Snappy используетсЯ по умолчанию, по этому его можно не указывать
df.write.format("avro").mode("overwrite").save("/content/data_out/task4")

#читаю созданный файл
spark.read.format("avro").option("compression", "Snappy").load("/content/data_out/task4").show(5,False)

#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#5. Ќапишите запрос, возвращающий фамилии и зарплаты всех сотрудников, которые подчинЯютсЯ сотруднику King. €спользуйте подзапрос.

df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_employees.createOrReplaceTempView("employees")

df = spark.sql("select e.last_name, e.salary from employees e where manager_id in\
                ( select employee_id from employees where last_name = 'King')")
df.show(20, False)

#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#6. Ќапишите запрос, возвращающий фамилии всех сотрудников, получающих больше, чем любой сотрудник отдела с ID 60.

df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_employees.createOrReplaceTempView("employees")

df = spark.sql("select Last_name from employees where salary > (select min(salary) from employees where department_id=60)")
df.show(20, False)


#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#7. Ќапишите запрос, возвращающий ID, фамилии и зарплаты всех сотрудников, работающих в одном отделе с работником, в чьей фамилии есть буква 'u' и получающих больше средней зарплаты в компании.

df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_employees.createOrReplaceTempView("employees")
df_departments = spark.read.format("csv").option("header","true")\
						   .option("inferSchema","true").option("delimiter",",")\
						   .load("/content/departments")
df_departments.createOrReplaceTempView("departments")
df = spark.sql("select employee_id, last_name, salary from employees where department_id in (select  department_id from employees where position('u' in lower(last_name)) > 0) \
                                                       and salary >	(select avg(salary) from employees )")
df.show(20, False)

#--zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz
#8. ‚ыведите фамилии, id отдела и название отдела длЯ всех сотрудников, не привЯзанных ни к одному отделу, а также список отделов, к которым не привЯзан ни один сотрудник.


df_employees = spark.read.format("csv").option("header","true")\
						 .option("inferSchema","true").option("delimiter","\t")\
						 .load("/content/employees")
df_departments = spark.read.format("csv").option("header","true")\
						   .option("inferSchema","true").option("delimiter",",")\
						   .load("/content/departments")
df_employees.createOrReplaceTempView("employees")
df_departments.createOrReplaceTempView("departments")


df = spark.sql("select last_name, department_id, null as department_name  from employees where department_id is null\
                union \
               select null as last_name, department_id, department_name from departments where department_id in ( select d.department_id\
                                            from departments d\
                                          except\
                                          select department_id\
                                           from employees e )")
df.show(20, False)